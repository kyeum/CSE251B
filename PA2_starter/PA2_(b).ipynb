{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "inp: (50000, 32, 32, 3)\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "from neuralnet_inside import *\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# Load the configuration.\n",
    "config = load_config(\"./data\")\n",
    "\n",
    "X_train, y_train, X_stats = load_data(path=\"./data\", stats=None, mode=\"train\")\n",
    "\n",
    "# Get 10 examples, 1 from each category.\n",
    "X_sub = []\n",
    "y_sub = []\n",
    "for k in range(10):\n",
    "    indices = y_train[:,k] == 1\n",
    "    X_sub.append(X_train[indices][0])\n",
    "    y_sub.append(y_train[indices][0])\n",
    "print(len(X_sub), len(y_sub))\n",
    "X_sub = np.array(X_sub)\n",
    "y_sub = np.array(y_sub)\n",
    "\n",
    "# Load model para\n",
    "config_prob_b = {}\n",
    "config_prob_b['layer_specs'] = [3072, 64,64, 10]\n",
    "config_prob_b['activation'] = 'tanh'\n",
    "config_prob_b['learning_rate'] = 0.01\n",
    "config_prob_b['batch_size'] = 128 \n",
    "config_prob_b['epochs'] = 100  \n",
    "config_prob_b['early_stop'] = True \n",
    "config_prob_b['early_stop_epoch'] = 5  \n",
    "config_prob_b['L2_penalty'] = 0  \n",
    "config_prob_b['momentum'] = True\n",
    "config_prob_b['momentum_gamma'] = 0.9  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b gradient:  0.0 numerical estimation:  0.0\n",
      "w1 gradient:  0.0 numerical estimation:  0.0\n",
      "w2 gradient:  -0.024485076445083118 numerical estimation:  0.024434207990253753\n",
      "b diff:  0.0\n",
      "w1 diff:  0.0\n",
      "w2 diff:  -5.0868454829364196e-05\n",
      "b gradient:  -0.004891853728864167 numerical estimation:  0.004892077853924093\n",
      "w1 gradient:  0.0048915870566938555 numerical estimation:  -0.004891810290175158\n",
      "w2 gradient:  7.363770472256348e-05 numerical estimation:  -7.353673225907187e-05\n",
      "b diff:  2.2412505992601417e-07\n",
      "w1 diff:  -2.2323348130258436e-07\n",
      "w2 diff:  1.0097246349161212e-07\n",
      "b gradient:  -0.009801115683931015 numerical estimation:  0.012915655211998\n",
      "w1 gradient:  0.20718969368826878 numerical estimation:  -0.20359102292921705\n",
      "w2 gradient:  0.16683971376113468 numerical estimation:  -0.16645665623649109\n",
      "b diff:  0.0031145395280669855\n",
      "w1 diff:  0.0035986707590517364\n",
      "w2 diff:  0.00038305752464359855\n"
     ]
    }
   ],
   "source": [
    "# d_b d_w comparison\n",
    "\n",
    "## Part (b) Estimation of bias weight and weight\n",
    "def Num_Est_b(model, layer, eps, output_idx):\n",
    "    layer.b[0][output_idx] += eps # E(w+e)\n",
    "    loss_1 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.b[0][output_idx] -= 2*eps # E(w-e)\n",
    "    loss_2 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.b[0][output_idx] += eps # back to normal\n",
    "    #print(\"diff:\", loss_1 - loss_2)\n",
    "    #print(\"loss:\", (loss_1 - loss_2) / (2 * eps))\n",
    "    return (loss_1 - loss_2) / (2 * eps)*10 # Numerical estimation for dEdW\n",
    "\n",
    "def Num_Est_w(model, layer, eps, input_idx, output_idx):\n",
    "    layer.w[input_idx][output_idx] += eps # E(w+e)\n",
    "    loss_1 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.w[input_idx][output_idx] -= 2*eps # E(w-e)\n",
    "    loss_2 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.w[input_idx][output_idx] += eps # back to normal\n",
    "    #print(\"diff:\", loss_1 - loss_2)\n",
    "    #print(\"loss:\", (loss_1 - loss_2) / (2 * eps))\n",
    "    return (loss_1 - loss_2) / (2 * eps)*10 # Numerical estimation for dEdW\n",
    "\n",
    "# Weights to modify:\n",
    "# 1 output bias weight\n",
    "# 1 hidden bias weight for each hidden layer\n",
    "# 2 hidden to output weights\n",
    "# 2 input to hidden weights\n",
    "# Show that the grad between is within O(eps^2) of backprop weights.\n",
    "db = []\n",
    "dw = []\n",
    "db_est = []\n",
    "dw_est = []\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "\n",
    "model = Neuralnetwork(config_prob_b)\n",
    "\n",
    "loss_train = model.forward(X_sub, targets=y_sub)\n",
    "model.backward()\n",
    "\n",
    "\n",
    "d_w_lst, d_b_lst = [], []\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, Layer):\n",
    "        d_b = Num_Est_b(model, layer,eps,0) # model, layer, eps, output_idx\n",
    "        d_w_1 = Num_Est_w(model, layer,eps,0,0) #model, layer, eps, output_idx\n",
    "        d_w_2 = Num_Est_w(model, layer,eps,0,1)\n",
    "        print('b gradient: ', layer.d_b[0], 'numerical estimation: ', d_b)\n",
    "        print('w1 gradient: ', layer.d_w[0][0], 'numerical estimation: ', d_w_1)\n",
    "        print('w2 gradient: ', layer.d_w[0][1], 'numerical estimation: ', d_w_2)\n",
    "        print('b diff: ', layer.d_b[0]+d_b)\n",
    "        print('w1 diff: ', layer.d_w[0][0]+d_w_1)\n",
    "        print('w2 diff: ', layer.d_w[0][1]+d_w_2)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Hidden Layer 1\n",
    "hidden_layer_1 = model_b.layers[0]\n",
    "# Hidden Layer 2\n",
    "hidden_layer_2 = model_b.layers[2]\n",
    "# Output Layer\n",
    "output_layer = model_b.layers[4]\n",
    "\n",
    "# 1 output bias weight\n",
    "input_idx = 0\n",
    "output_idx = 1\n",
    "cur_layer = output_layer\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "print(cur_layer.d_b.shape)\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Output bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 1 hidden bias weight for hidden layer 2\n",
    "cur_layer = hidden_layer_2\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Hidden Layer 2 bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 1 hidden bias weight for hidden layer 1\n",
    "cur_layer = hidden_layer_1\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Hidden Layer 1 bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 2 hidden to output weights\n",
    "input_idx = 0\n",
    "output_idx = 1\n",
    "cur_layer = hidden_layer_2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(\"actual:\", dw[-1])\n",
    "print(\"est:\", dw_est[-1])\n",
    "print(f\"Hidden to output weight diff 1: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "input_idx = 0\n",
    "output_idx = 2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Hidden to output weight diff 2: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "\n",
    "# 2 input to hidden weights\n",
    "input_idx = 0\n",
    "output_idx = 0\n",
    "cur_layer = hidden_layer_1\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Input to hidden 1 weight diff 1: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "input_idx = 0\n",
    "output_idx = 2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Input to hidden 2 weight diff 2: {dw_est[-1] - dw[-1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_b d_w comparison\n",
    "\n",
    "## Part (b) Estimation of bias weight and weight\n",
    "def Num_Est_b(model, layer, eps, output_idx):\n",
    "    layer.b[0][output_idx] += eps # E(w+e)\n",
    "    loss_1 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.b[0][output_idx] -= 2*eps # E(w-e)\n",
    "    loss_2 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.b[0][output_idx] += eps # back to normal\n",
    "    print(\"diff:\", loss_1 - loss_2)\n",
    "    print(\"loss:\", (loss_1 - loss_2) / (2 * eps))\n",
    "    return (loss_1 - loss_2) / (2 * eps) # Numerical estimation for dEdW\n",
    "\n",
    "def Num_Est_w(model, layer, eps, input_idx, output_idx):\n",
    "    layer.w[input_idx][output_idx] += eps # E(w+e)\n",
    "    loss_1 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.w[input_idx][output_idx] -= 2*eps # E(w-e)\n",
    "    loss_2 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.w[input_idx][output_idx] += eps # back to normal\n",
    "    print(\"diff:\", loss_1 - loss_2)\n",
    "    print(\"loss:\", (loss_1 - loss_2) / (2 * eps))\n",
    "    return (loss_1 - loss_2) / (2 * eps) # Numerical estimation for dEdW\n",
    "\n",
    "print(\"Layers:\", len(model_b.layers))\n",
    "# Weights to modify:\n",
    "# 1 output bias weight\n",
    "# 1 hidden bias weight for each hidden layer\n",
    "# 2 hidden to output weights\n",
    "# 2 input to hidden weights\n",
    "# Show that the grad between is within O(eps^2) of backprop weights.\n",
    "db = []\n",
    "dw = []\n",
    "db_est = []\n",
    "dw_est = []\n",
    "\n",
    "eps = 0.0001\n",
    "# Hidden Layer 1\n",
    "hidden_layer_1 = model_b.layers[0]\n",
    "# Hidden Layer 2\n",
    "hidden_layer_2 = model_b.layers[2]\n",
    "# Output Layer\n",
    "output_layer = model_b.layers[4]\n",
    "\n",
    "# 1 output bias weight\n",
    "input_idx = 0\n",
    "output_idx = 1\n",
    "cur_layer = output_layer\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "print(cur_layer.d_b.shape)\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Output bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 1 hidden bias weight for hidden layer 2\n",
    "cur_layer = hidden_layer_2\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Hidden Layer 2 bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 1 hidden bias weight for hidden layer 1\n",
    "cur_layer = hidden_layer_1\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Hidden Layer 1 bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 2 hidden to output weights\n",
    "input_idx = 0\n",
    "output_idx = 1\n",
    "cur_layer = hidden_layer_2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(\"actual:\", dw[-1])\n",
    "print(\"est:\", dw_est[-1])\n",
    "print(f\"Hidden to output weight diff 1: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "input_idx = 0\n",
    "output_idx = 2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Hidden to output weight diff 2: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "\n",
    "# 2 input to hidden weights\n",
    "input_idx = 0\n",
    "output_idx = 0\n",
    "cur_layer = hidden_layer_1\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Input to hidden 1 weight diff 1: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "input_idx = 0\n",
    "output_idx = 2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Input to hidden 2 weight diff 2: {dw_est[-1] - dw[-1]:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "005f4ed24810181fa8b2266ec303702d6575bedfaa41c7848d327e7f780b3129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

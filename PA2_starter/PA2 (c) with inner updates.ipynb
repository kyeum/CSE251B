{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (c) - Update inside of batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "inp: (50000, 32, 32, 3)\n",
      " epoch: 1, train accuracy: 0.2429, train_loss_norm:0.3098, valid_acc: 0.2662, valid_loss_norm: 0.2055\n",
      " epoch: 2, train accuracy: 0.2866, train_loss_norm:0.2033, valid_acc: 0.2696, valid_loss_norm: 0.2031\n",
      " epoch: 3, train accuracy: 0.2983, train_loss_norm:0.2003, valid_acc: 0.2872, valid_loss_norm: 0.2001\n",
      " epoch: 4, train accuracy: 0.3052, train_loss_norm:0.1984, valid_acc: 0.2834, valid_loss_norm: 0.1995\n",
      " epoch: 5, train accuracy: 0.3118, train_loss_norm:0.1962, valid_acc: 0.2930, valid_loss_norm: 0.1975\n",
      " epoch: 6, train accuracy: 0.3158, train_loss_norm:0.1958, valid_acc: 0.2912, valid_loss_norm: 0.1978\n",
      "patience cnt 1\n",
      " epoch: 7, train accuracy: 0.3248, train_loss_norm:0.1936, valid_acc: 0.2890, valid_loss_norm: 0.1966\n",
      " epoch: 8, train accuracy: 0.3284, train_loss_norm:0.1925, valid_acc: 0.2892, valid_loss_norm: 0.1960\n",
      " epoch: 9, train accuracy: 0.3328, train_loss_norm:0.1913, valid_acc: 0.3114, valid_loss_norm: 0.1931\n",
      " epoch: 10, train accuracy: 0.3378, train_loss_norm:0.1903, valid_acc: 0.3014, valid_loss_norm: 0.1941\n",
      "patience cnt 1\n",
      " epoch: 11, train accuracy: 0.3366, train_loss_norm:0.1898, valid_acc: 0.3152, valid_loss_norm: 0.1961\n",
      "patience cnt 2\n",
      " epoch: 12, train accuracy: 0.3426, train_loss_norm:0.1890, valid_acc: 0.3102, valid_loss_norm: 0.1931\n",
      " epoch: 13, train accuracy: 0.3478, train_loss_norm:0.1877, valid_acc: 0.3194, valid_loss_norm: 0.1925\n",
      " epoch: 14, train accuracy: 0.3486, train_loss_norm:0.1874, valid_acc: 0.3016, valid_loss_norm: 0.1932\n",
      "patience cnt 1\n",
      " epoch: 15, train accuracy: 0.3523, train_loss_norm:0.1861, valid_acc: 0.3168, valid_loss_norm: 0.1931\n",
      "patience cnt 2\n",
      " epoch: 16, train accuracy: 0.3538, train_loss_norm:0.1857, valid_acc: 0.3206, valid_loss_norm: 0.1911\n",
      " epoch: 17, train accuracy: 0.3558, train_loss_norm:0.1852, valid_acc: 0.3224, valid_loss_norm: 0.1910\n",
      " epoch: 18, train accuracy: 0.3577, train_loss_norm:0.1847, valid_acc: 0.3206, valid_loss_norm: 0.1913\n",
      "patience cnt 1\n",
      " epoch: 19, train accuracy: 0.3610, train_loss_norm:0.1842, valid_acc: 0.3134, valid_loss_norm: 0.1925\n",
      "patience cnt 2\n",
      " epoch: 20, train accuracy: 0.3614, train_loss_norm:0.1839, valid_acc: 0.3210, valid_loss_norm: 0.1901\n",
      " epoch: 21, train accuracy: 0.3614, train_loss_norm:0.1830, valid_acc: 0.3236, valid_loss_norm: 0.1895\n",
      " epoch: 22, train accuracy: 0.3669, train_loss_norm:0.1821, valid_acc: 0.3216, valid_loss_norm: 0.1894\n",
      " epoch: 23, train accuracy: 0.3649, train_loss_norm:0.1825, valid_acc: 0.3132, valid_loss_norm: 0.1919\n",
      "patience cnt 1\n",
      " epoch: 24, train accuracy: 0.3688, train_loss_norm:0.1816, valid_acc: 0.3250, valid_loss_norm: 0.1874\n",
      " epoch: 25, train accuracy: 0.3664, train_loss_norm:0.1814, valid_acc: 0.3078, valid_loss_norm: 0.1898\n",
      "patience cnt 1\n",
      " epoch: 26, train accuracy: 0.3732, train_loss_norm:0.1802, valid_acc: 0.3334, valid_loss_norm: 0.1870\n",
      " epoch: 27, train accuracy: 0.3745, train_loss_norm:0.1803, valid_acc: 0.3274, valid_loss_norm: 0.1891\n",
      "patience cnt 1\n",
      " epoch: 28, train accuracy: 0.3771, train_loss_norm:0.1791, valid_acc: 0.3416, valid_loss_norm: 0.1873\n",
      "patience cnt 2\n",
      " epoch: 29, train accuracy: 0.3783, train_loss_norm:0.1790, valid_acc: 0.3324, valid_loss_norm: 0.1884\n",
      "patience cnt 3\n",
      " epoch: 30, train accuracy: 0.3808, train_loss_norm:0.1788, valid_acc: 0.3388, valid_loss_norm: 0.1873\n",
      "patience cnt 4\n",
      " epoch: 31, train accuracy: 0.3818, train_loss_norm:0.1780, valid_acc: 0.3386, valid_loss_norm: 0.1856\n",
      " epoch: 32, train accuracy: 0.3849, train_loss_norm:0.1771, valid_acc: 0.3332, valid_loss_norm: 0.1869\n",
      "patience cnt 1\n",
      " epoch: 33, train accuracy: 0.3869, train_loss_norm:0.1771, valid_acc: 0.3436, valid_loss_norm: 0.1878\n",
      "patience cnt 2\n",
      " epoch: 34, train accuracy: 0.3850, train_loss_norm:0.1773, valid_acc: 0.3308, valid_loss_norm: 0.1887\n",
      "patience cnt 3\n",
      " epoch: 35, train accuracy: 0.3879, train_loss_norm:0.1771, valid_acc: 0.3504, valid_loss_norm: 0.1884\n",
      "patience cnt 4\n",
      " epoch: 36, train accuracy: 0.3906, train_loss_norm:0.1760, valid_acc: 0.3384, valid_loss_norm: 0.1867\n",
      "patience cnt 5\n",
      "earlystop\n"
     ]
    }
   ],
   "source": [
    "# from neuralnet_update_PC import *\n",
    "# from neuralnet_works_FC import *\n",
    "from neuralnet_inside import *\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "config_c = {}\n",
    "config_c['layer_specs'] = [3072, 64, 64, 10]\n",
    "config_c['activation'] = 'tanh'\n",
    "config_c['learning_rate'] = 0.005\n",
    "config_c['batch_size'] = 128\n",
    "config_c['epochs'] = 100  \n",
    "config_c['early_stop'] = True \n",
    "config_c['early_stop_epoch'] = 5\n",
    "config_c['L2_penalty'] = 0  \n",
    "config_c['momentum'] = True  \n",
    "config_c['momentum_gamma'] = 0.9  \n",
    "# Create the model\n",
    "\n",
    "#TODO\n",
    "#momentum, early stop , expect accuracy around 37%\n",
    "\n",
    "\"\"\"\n",
    "Train your model here.\n",
    "Implement batch SGD to train the model.\n",
    "Implement Early Stopping.\n",
    "Use config to set parameters for training like learning rate, momentum, etc.\n",
    "\"\"\"\n",
    "def train_e(model, x_train, y_train, x_valid, y_valid, config):\n",
    "    epochs = config['epochs']\n",
    "    batch_size = config['batch_size']\n",
    "    momentum =    config['momentum']\n",
    "    momentum_gamma = config['momentum_gamma']\n",
    "    L2_penalty = config['momentum_gamma']\n",
    "    patience = config['early_stop_epoch']\n",
    "\n",
    "    train_loss_record = []\n",
    "    train_accuracy_record = []\n",
    "    holdout_loss_record = []\n",
    "    holdout_accuracy_record = []\n",
    "    \n",
    "    min_val_Loss = float('inf')\n",
    "    epoch_stop = 0\n",
    "\n",
    "    # How many times the validation loss has gone up in a row.\n",
    "    cur_loss_up_sequence = 0\n",
    "\n",
    "    print(\"num_mini-batches\", len(x_train) / batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_stop = epoch\n",
    "        batch_loss = []\n",
    "        batch_accuracy = []\n",
    "        for x, y in generate_minibatches(x_train, y_train, batch_size):\n",
    "            # Forward Pass\n",
    "            train_y, loss = model.forward(x, y)\n",
    "            batch_loss.append(loss) \n",
    "            # Backward Pass\n",
    "            model.backward()\n",
    "            model.updateweight() # update weight for each layer.\n",
    "            batch_accuracy.append(model.accuracy(x,y))\n",
    "\n",
    "        #model.updateweight() # update weight for each layer.\n",
    "\n",
    "        #model.zero_grad() added inside of updateweight\n",
    "\n",
    "        #overall loss calculation\n",
    "        #y_hat, loss = model.forward(x_train, y_train)\n",
    "        #acc = model.accuracy(x_train, y_train)\n",
    "        \n",
    "        train_loss = np.mean(np.array(batch_loss))\n",
    "        train_accuracy = np.mean(np.array(batch_accuracy))\n",
    "\n",
    "        #train_loss = np.mean(np.array(loss))\n",
    "        #train_accuracy = np.mean(np.array(acc))\n",
    "        \n",
    "        holdout_loss = model.forward(x_valid, y_valid)[1]\n",
    "        holdout_accuracy = model.accuracy(x_valid, y_valid)\n",
    "\n",
    "        train_loss_record.append(train_loss)\n",
    "        train_accuracy_record.append(train_accuracy)\n",
    "\n",
    "        holdout_loss_record.append(holdout_loss)\n",
    "        holdout_accuracy_record.append(holdout_accuracy)\n",
    "\n",
    "        print(f' epoch: {epoch + 1}, train accuracy: {train_accuracy:.4f}, train_loss_norm:{train_loss:.4f}, '\\\n",
    "            f'valid_acc: {holdout_accuracy:.4f}, valid_loss_norm: {holdout_loss:.4f}')   \n",
    "                \n",
    "        \n",
    "        # Save the best weights according to test set.\n",
    "        if holdout_loss > min_val_Loss:\n",
    "            cur_loss_up_sequence += 1\n",
    "            print(\"patience cnt\",cur_loss_up_sequence)\n",
    "\n",
    "            if cur_loss_up_sequence >= patience:\n",
    "                print(\"earlystop\")\n",
    "                break\n",
    "        else:\n",
    "            min_val_Loss = holdout_loss\n",
    "            cur_loss_up_sequence = 0\n",
    "            # Save the best weights.\n",
    "            model.save_load_weight(save=True)\n",
    "    \n",
    "    return epoch_stop, train_loss_record, train_accuracy_record, holdout_loss_record, holdout_accuracy_record\n",
    "\n",
    "model_c  = Neuralnetwork(config_c)\n",
    "\n",
    "# Load the data\n",
    "x_train, y_train, stats = load_data(path=\"./data\",stats = None, mode=\"train\")\n",
    "x_test, y_test = load_data(path=\"./data\",stats = stats, mode=\"test\")\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = split_data(x_train,y_train)\n",
    "\n",
    "epoch, train_loss_record, train_accuracy_record, holdout_loss_record, holdout_accuracy_record = train(model_c,x_train,y_train,x_valid,y_valid,config_c)\n",
    "\n",
    "# Recall parameters with minimum validation loss\n",
    "model_c.save_load_weight(save=False) # load data\n",
    "test_accuracy = test(model_c, x_test, y_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set result :  0.3487\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (36,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wt/l4n6hhns3dz9d7ljspfyxvd40000gn/T/ipykernel_42492/3297742744.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout_loss_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (36,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('test set result : ' ,test_accuracy)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(np.arange(epoch+1), train_loss_record, label='train')\n",
    "plt.plot(np.arange(epoch+1), holdout_loss_record, label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and validation loss vs epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(np.arange(epoch+1), train_accuracy_record, label='train')\n",
    "plt.plot(np.arange(epoch+1), holdout_accuracy_record, label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and validation accuracy vs epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification (d) - Update inside of batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralnet_update_PC import *\n",
    "# from neuralnet_works_FC import *\n",
    "from neuralnet_inside import *\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "config_d = {}\n",
    "config_d['layer_specs'] = [3072, 64, 64, 10]\n",
    "config_d['activation'] = 'tanh'\n",
    "config_d['learning_rate'] = 0.005\n",
    "config_d['batch_size'] = 128\n",
    "config_d['epochs'] = 110   # need to 10% more from configc\n",
    "config_d['early_stop'] = True \n",
    "config_d['early_stop_epoch'] = 10\n",
    "config_d['L2_penalty'] = 0  \n",
    "config_d['momentum'] = True  \n",
    "config_d['momentum_gamma'] = 0.9  \n",
    "# Create the model\n",
    "\n",
    "#TODO\n",
    "#momentum, early stop , expect accuracy around 37%\n",
    "\n",
    "\"\"\"\n",
    "Train your model here.\n",
    "Implement batch SGD to train the model.\n",
    "Implement Early Stopping.\n",
    "Use config to set parameters for training like learning rate, momentum, etc.\n",
    "\"\"\"\n",
    "def train_e(model, x_train, y_train, x_valid, y_valid, config):\n",
    "    epochs = config['epochs']\n",
    "    batch_size = config['batch_size']\n",
    "    momentum =    config['momentum']\n",
    "    momentum_gamma = config['momentum_gamma']\n",
    "    L2_penalty = config['momentum_gamma']\n",
    "    patience = config['early_stop_epoch']\n",
    "\n",
    "    train_loss_record = []\n",
    "    train_accuracy_record = []\n",
    "    holdout_loss_record = []\n",
    "    holdout_accuracy_record = []\n",
    "    \n",
    "    min_val_Loss = float('inf')\n",
    "    epoch_stop = 0\n",
    "\n",
    "    # How many times the validation loss has gone up in a row.\n",
    "    cur_loss_up_sequence = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_stop = epoch\n",
    "        batch_loss = []\n",
    "        batch_accuracy = []\n",
    "        for x, y in generate_minibatches(x_train, y_train, batch_size):\n",
    "            # Forward Pass\n",
    "            train_y, loss = model.forward(x, y)\n",
    "            batch_loss.append(loss) \n",
    "            # Backward Pass\n",
    "            model.backward()\n",
    "            model.updateweight() # update weight for each layer.\n",
    "            batch_accuracy.append(model.accuracy(x,y))\n",
    "\n",
    "\n",
    "        train_loss = np.mean(np.array(batch_loss))\n",
    "        train_accuracy = np.mean(np.array(batch_accuracy))\n",
    "        \n",
    "        #model.updateweight() # update weight for each layer.\n",
    "        #y_hat, loss = model.forward(x_train, y_train)\n",
    "        #acc = model.accuracy(x_train, y_train)\n",
    "        \n",
    "        #train_loss = np.mean(np.array(loss))\n",
    "        #train_accuracy = np.mean(np.array(acc))\n",
    "        \n",
    "        holdout_loss = model.forward(x_valid, y_valid)[1]\n",
    "        holdout_accuracy = model.accuracy(x_valid, y_valid)\n",
    "\n",
    "        train_loss_record.append(train_loss)\n",
    "        train_accuracy_record.append(train_accuracy)\n",
    "\n",
    "        holdout_loss_record.append(holdout_loss)\n",
    "        holdout_accuracy_record.append(holdout_accuracy)\n",
    "\n",
    "        print(f' epoch: {epoch + 1}, train accuracy: {train_accuracy:.4f}, train_loss_norm:{train_loss:.4f}, '\\\n",
    "            f'valid_acc: {holdout_accuracy:.4f}, valid_loss_norm: {holdout_loss:.4f}')   \n",
    "                \n",
    "        \n",
    "        # Save the best weights according to test set.\n",
    "        if holdout_loss > min_val_Loss:\n",
    "            cur_loss_up_sequence += 1\n",
    "            print(\"patience cnt\",cur_loss_up_sequence)\n",
    "\n",
    "            if cur_loss_up_sequence >= patience:\n",
    "                print(\"earlystop\")\n",
    "                break\n",
    "        else:\n",
    "            min_val_Loss = holdout_loss\n",
    "            cur_loss_up_sequence = 0\n",
    "            # Save the best weights.\n",
    "            model.save_load_weight(save=True)\n",
    "    \n",
    "    return epoch_stop, train_loss_record, train_accuracy_record, holdout_loss_record, holdout_accuracy_record\n",
    "\n",
    "L2 = [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "\n",
    "# Load the data\n",
    "x_train, y_train, stats = load_data(path=\"./data\",stats = None, mode=\"train\")\n",
    "x_test, y_test = load_data(path=\"./data\",stats = stats, mode=\"test\")\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = split_data(x_train,y_train)\n",
    "# model add\n",
    "\n",
    "epoch_all = []\n",
    "train_loss_record_all = []\n",
    "holdout_loss_record_all = []\n",
    "train_accuracy_record_all = []\n",
    "holdout_accuracy_record_all = []\n",
    "test_accuracy_all = []\n",
    "    \n",
    "for var in L2:\n",
    "    config_d['L2_penalty'] = var\n",
    "    model_d  = Neuralnetwork(config_d)\n",
    "    epoch, train_loss_record, train_accuracy_record, holdout_loss_record, holdout_accuracy_record = train_e(model_d,x_train,y_train,x_valid,y_valid,config_d)\n",
    "    \n",
    "    epoch_all.append(epoch)\n",
    "    train_loss_record_all.append(train_loss_record)\n",
    "    holdout_loss_record_all.append(holdout_loss_record)\n",
    "    train_accuracy_record_all.append(train_accuracy_record)\n",
    "    holdout_accuracy_record_all.append(holdout_accuracy_record)    \n",
    "    model_d.save_load_weight(save=False) # load data\n",
    "    test_accuracy = test(model_d, x_test, y_test) \n",
    "    test_accuracy_all.append(test_accuracy)                          \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test set result : ' ,test_accuracy_all)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(np.arange(epoch_all[0]+1), train_loss_record_all[0], label='train')\n",
    "plt.plot(np.arange(epoch_all[0]+1), holdout_loss_record_all[0], label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and validation loss vs epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(np.arange(epoch_all[0]+1), train_accuracy_record_all[0], label='train')\n",
    "plt.plot(np.arange(epoch_all[0]+1), holdout_accuracy_record_all[0], label='valid')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and validation accuracy vs epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(3)\n",
    "plt.plot(np.arange(epoch_all[1]+1), train_loss_record_all[1], label='train')\n",
    "plt.plot(np.arange(epoch_all[1]+1), holdout_loss_record_all[1], label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and validation loss vs epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(np.arange(epoch_all[1]+1), train_accuracy_record_all[1], label='train')\n",
    "plt.plot(np.arange(epoch_all[1]+1), holdout_accuracy_record_all[1], label='valid')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and validation accuracy vs epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(np.arange(epoch_all[2]+1), train_loss_record_all[2], label='train')\n",
    "plt.plot(np.arange(epoch_all[2]+1), holdout_loss_record_all[2], label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and validation loss vs epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(6)\n",
    "plt.plot(np.arange(epoch_all[2]+1), train_accuracy_record_all[2], label='train')\n",
    "plt.plot(np.arange(epoch_all[2]+1), holdout_accuracy_record_all[2], label='valid')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and validation accuracy vs epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "005f4ed24810181fa8b2266ec303702d6575bedfaa41c7848d327e7f780b3129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

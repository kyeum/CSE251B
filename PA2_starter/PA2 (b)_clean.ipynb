{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "inp: (50000, 32, 32, 3)\n",
      "10 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.16256776,  -8.88166446,   4.65124877, ...,  -5.20356103,\n",
       "          1.73638568,  10.69495567],\n",
       "       [ 16.8490327 , -33.59014221,   6.97508011, ..., -20.94218458,\n",
       "         -4.15596622,  29.36727404],\n",
       "       [ -3.59779998,  -0.10507974,  -1.94844756, ...,  15.13638649,\n",
       "         -0.88855443, -16.11482895],\n",
       "       ...,\n",
       "       [ -7.91144485,  13.48843793,  -4.52109209, ...,   9.44241916,\n",
       "         -4.96564945, -17.51147168],\n",
       "       [  4.99169585, -10.6426176 ,  21.1392499 , ...,   4.1516606 ,\n",
       "          6.86621007, -12.68241444],\n",
       "       [ -9.03211193,  12.12942032,   1.60684442, ...,   6.27620176,\n",
       "         13.28047936,  -6.25968368]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neuralnet import *\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# Load the configuration.\n",
    "config = load_config(\"./data\")\n",
    "\n",
    "# Create the model\n",
    "#model  = Neuralnetwork(config)\n",
    "# Load the data\n",
    "#x_train, y_train = load_data(path=\"./data\", mode=\"train\") # output normalized and one hot encoding\n",
    "#x_test, y_test = load_data(path=\"./data\", mode=\"test\")# output normalized and one hot encoding\n",
    "\n",
    "# Use 10 examples one from each category\n",
    "'''\n",
    "# Data loading and spliting\n",
    "X_b, y_b, X_stats = load_data(path=\"./data\", mode=\"train\")\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "class_num = list(range(10))\n",
    "\n",
    "\n",
    "X_train, y_train = [], []\n",
    "for idx in range(y_b.shape[0]):\n",
    "    if len(class_num) == 0:\n",
    "        break\n",
    "    indice = onehot_decode(y_b[idx])\n",
    "    \n",
    "    if indice in class_num:\n",
    "        X_train.append(X_b[idx])\n",
    "        y_train.append(y_b[idx])\n",
    "        class_num.remove(indice)\n",
    "        \n",
    "        \n",
    "X_train = np.array(X_train)        \n",
    "y_train = np.array(y_train)       \n",
    "'''\n",
    "\n",
    "X_train, y_train, X_stats = load_data(path=\"./data\", stats=None, mode=\"train\")\n",
    "\n",
    "# Get 10 examples, 1 from each category.\n",
    "X_sub = []\n",
    "y_sub = []\n",
    "for k in range(10):\n",
    "    indices = y_train[:,k] == 1\n",
    "    X_sub.append(X_train[indices][0])\n",
    "    y_sub.append(y_train[indices][0])\n",
    "print(len(X_sub), len(y_sub))\n",
    "X_sub = np.array(X_sub)\n",
    "y_sub = np.array(y_sub)\n",
    "\n",
    "# Load model para\n",
    "config_prob_b = {}\n",
    "config_prob_b['layer_specs'] = [3072, 10, 10, 10]\n",
    "config_prob_b['activation'] = 'tanh'\n",
    "config_prob_b['learning_rate'] = 0.005\n",
    "config_prob_b['batch_size'] = 128 \n",
    "config_prob_b['epochs'] = 100  \n",
    "config_prob_b['early_stop'] = True \n",
    "config_prob_b['early_stop_epoch'] = 5  \n",
    "config_prob_b['L2_penalty'] = 0  \n",
    "config_prob_b['momentum'] = False  \n",
    "config_prob_b['momentum_gamma'] = 0.9  \n",
    "\n",
    "model_b = Neuralnetwork(config_prob_b)\n",
    "model_b.forward(X_sub, y_sub)\n",
    "model_b.backward()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 5\n",
      "2.7955808287458708\n",
      "2.804619725137625\n",
      "Output bias weight diff: 0.000050\n",
      "2.81047141362105\n",
      "2.787648238977348\n",
      "Hidden Layer 2 bias weight diff: -0.003901\n",
      "2.7998624079701417\n",
      "2.7998825342605023\n",
      "Hidden Layer 2 bias weight diff: 1.890356\n",
      "Hidden to output weight diff 1: 0.596210\n",
      "Hidden to output weight diff 2: -0.160733\n"
     ]
    }
   ],
   "source": [
    "# d_b d_w comparison\n",
    "\n",
    "## Part (b) Estimation of bias weight and weight\n",
    "def Num_Est_b(model, layer, eps, output_idx):\n",
    "    layer.b[0][output_idx] += eps # E(w+e)\n",
    "    loss_1 = model.forward(X_sub, y_sub)[1]\n",
    "    print(loss_1)\n",
    "    layer.b[0][output_idx] -= 2*eps # E(w-e)\n",
    "    loss_2 = model.forward(X_sub, y_sub)[1]\n",
    "    print(loss_2)\n",
    "    layer.b[0][output_idx] += eps # back to normal\n",
    "    return (loss_1 - loss_2) / (2 * eps) # Numerical estimation for dEdW\n",
    "\n",
    "def Num_Est_w(model, layer, eps, input_idx, output_idx):\n",
    "    layer.w[input_idx][output_idx] += eps # E(w+e)\n",
    "    loss_1 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.w[input_idx][output_idx] -= 2*eps # E(w-e)\n",
    "    loss_2 = model.forward(X_sub, y_sub)[1]\n",
    "    layer.w[input_idx][output_idx] += eps # back to normal\n",
    "    return (loss_1 - loss_2) / (2 * eps) # Numerical estimation for dEdW\n",
    "\n",
    "print(\"Layers:\", len(model_b.layers))\n",
    "# Weights to modify:\n",
    "# 1 output bias weight\n",
    "# 1 hidden bias weight for each hidden layer\n",
    "# 2 hidden to output weights\n",
    "# 2 input to hidden weights\n",
    "# Show that the grad between is within O(eps^2) of backprop weights.\n",
    "db = []\n",
    "dw = []\n",
    "db_est = []\n",
    "dw_est = []\n",
    "\n",
    "eps = 10e-2\n",
    "# Hidden Layer 1\n",
    "hidden_layer_1 = model_b.layers[0]\n",
    "# Hidden Layer 2\n",
    "hidden_layer_2 = model_b.layers[2]\n",
    "# Output Layer\n",
    "output_layer = model_b.layers[4]\n",
    "\n",
    "# 1 output bias weight\n",
    "input_idx = 0\n",
    "output_idx = 1\n",
    "cur_layer = output_layer\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Output bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 1 hidden bias weight for hidden layer 2\n",
    "cur_layer = hidden_layer_2\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Hidden Layer 2 bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 1 hidden bias weight for hidden layer 1\n",
    "cur_layer = hidden_layer_1\n",
    "db_est.append(Num_Est_b(model_b, cur_layer, eps, output_idx))\n",
    "db.append(cur_layer.d_b[0][output_idx]) \n",
    "print(f\"Hidden Layer 1 bias weight diff: {db_est[-1] - db[-1]:.6f}\")\n",
    "\n",
    "# 2 hidden to output weights\n",
    "input_idx = 0\n",
    "output_idx = 1\n",
    "cur_layer = hidden_layer_2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Hidden to output weight diff 1: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "input_idx = 0\n",
    "output_idx = 2\n",
    "dw_est.append(Num_Est_w(model_b, cur_layer, eps, input_idx, output_idx))\n",
    "dw.append(cur_layer.d_w[input_idx][output_idx])\n",
    "print(f\"Hidden to output weight diff 2: {dw_est[-1] - dw[-1]:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "005f4ed24810181fa8b2266ec303702d6575bedfaa41c7848d327e7f780b3129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

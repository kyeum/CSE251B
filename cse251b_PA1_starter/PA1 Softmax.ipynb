{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pca import PCA\n",
    "import argparse\n",
    "import network\n",
    "import os, random, sys\n",
    "from data import traffic_sign, generate_k_fold_set, onehot_encode, onehot_decode, z_score_normalize, append_bias\n",
    "from model.softmax import SoftmaxRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q6(a) - Evaluate Network on all 43 traffic signs (aligned dataset)\n",
    "\n",
    "# Load aligned data\n",
    "X, y = traffic_sign(True)\n",
    "X = X.astype(np.float32) # cast to float32 as float64 running out of memory\n",
    "X = z_score_normalize(X) \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X.dtype)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (i) With PCA on aligned\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "# Softmax Regression Parameters\n",
    "lr = 0.5\n",
    "num_features = X.shape[1]\n",
    "num_classes = y.max() + 1\n",
    "\n",
    "train_loss_record = []\n",
    "\n",
    "train_accuracy_record = []\n",
    "\n",
    "holdout_loss_record = []\n",
    "\n",
    "holdout_accuracy_record = []\n",
    "\n",
    "test_accuracy_record = []\n",
    "\n",
    "# PCA number of principal components\n",
    "n_components = 300\n",
    "\n",
    "first_plot = True\n",
    "\n",
    "num_epochs = 300\n",
    "epochs_print = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "k = 10\n",
    "\n",
    "total_test_accuracy = 0.0\n",
    "total_test_loss = 0.0\n",
    "\n",
    "cur_fold = 0\n",
    "for train, valid, test in generate_k_fold_set((X, y), k):\n",
    "    print(f\"Current Fold: {cur_fold}\")\n",
    "    train_data, train_label = train\n",
    "    valid_data, valid_label = valid\n",
    "    test_data, test_label = test\n",
    "    \n",
    "    # Project data onto principal components\n",
    "    pca = PCA(n_components)\n",
    "    projected = pca.fit_transform(train_data) # len(train_data) x n_components\n",
    "    \n",
    "    # Plot principal components\n",
    "    if first_plot == True : \n",
    "        pca.plot_PC()\n",
    "        first_plot = False\n",
    "    train_d = append_bias(projected)     \n",
    "    valid_d = append_bias(pca.PCA_generate(valid_data))\n",
    "    test_d = append_bias(pca.PCA_generate(test_data))\n",
    "\n",
    "    softmax_model = SoftmaxRegression(lr, n_components, num_classes)\n",
    "    best_w = softmax_model.W\n",
    "\n",
    "    # Onehot encode labels\n",
    "    y_true = onehot_encode(train_label)\n",
    "    valid_label_onehot = onehot_encode(valid_label)\n",
    "    test_label_onehot = onehot_encode(test_label)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_hat = softmax_model.model(train_d)\n",
    "        \n",
    "        raw_train_loss = softmax_model.cross_entropy(y_true, y_hat)\n",
    "        train_loss_norm = raw_train_loss / len(train_d) / num_classes # train loss normalized by # examples and classes\n",
    "        \n",
    "        train_loss_record.append(train_loss_norm)\n",
    "        \n",
    "        train_accuracy = softmax_model.accuracy(y_true, y_hat)\n",
    "        train_accuracy_record.append(train_accuracy)\n",
    "        \n",
    "        holdout_y = softmax_model.model(valid_d)\n",
    "\n",
    "        holdout_loss = softmax_model.cross_entropy(holdout_y, valid_label_onehot)\n",
    "        holdout_loss_norm = holdout_loss / len(valid_d) / num_classes # holdout loss normalized by # examples and classes\n",
    "        holdout_loss_record.append(holdout_loss_norm)\n",
    "\n",
    "        holdout_accuracy = softmax_model.accuracy(holdout_y, valid_label_onehot)\n",
    "        holdout_accuracy_record.append(holdout_accuracy)\n",
    "\n",
    "        if holdout_accuracy >= max(holdout_accuracy_record[cur_fold * num_epochs:]):\n",
    "            best_w = softmax_model.W\n",
    "\n",
    "        # Update Weights\n",
    "        softmax_model.update_weights(train_d, y_true, y_hat)\n",
    "\n",
    "        if (epoch + 1) in epochs_print:\n",
    "            print(f' epoch: {epoch + 1}, train accuracy: {train_accuracy:.4f}, train_loss_norm:{train_loss_norm:.4f}, '\\\n",
    "                f'valid_acc: {holdout_accuracy:.4f}, valid_loss_norm: {holdout_loss_norm:.4f}')\n",
    "            if DEBUG:\n",
    "                test_y = softmax_model.model_w(test_d, best_w)\n",
    "                test_y_1 = softmax_model.model(test_d)\n",
    "\n",
    "                test_accuracy = softmax_model.accuracy(test_y, test_label_onehot)\n",
    "                test_accuracy_1 = softmax_model.accuracy(test_y_1, test_label_onehot)\n",
    "\n",
    "                raw_test_loss = softmax_model.cross_entropy(test_y, test_label_onehot)\n",
    "                test_loss_norm = raw_test_loss / len(test_d) / num_classes\n",
    "\n",
    "                print(f'MODEL_W: Test accuracy: {test_accuracy:.4f}', f'Test loss norm: {test_loss_norm:.4f}')\n",
    "                print(f'MODEL: Test accuracy: {test_accuracy_1:.4f}')\n",
    "\n",
    "    # Run on Test Dataset\n",
    "    test_y = softmax_model.model_w(test_d, best_w)\n",
    "\n",
    "    test_accuracy = softmax_model.accuracy(test_y, test_label_onehot)\n",
    "\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    test_accuracy_record.append(test_accuracy)\n",
    "\n",
    "    raw_test_loss = softmax_model.cross_entropy(test_y, test_label_onehot)\n",
    "    test_loss_norm = raw_test_loss / len(test_d) / num_classes\n",
    "    total_test_loss += test_loss_norm\n",
    "    print(f\"Test loss norm: {test_loss_norm:.4f}\")\n",
    "\n",
    "    cur_fold += 1\n",
    "\n",
    "print(f'Average test accuracy over {k} folds: {np.mean(test_accuracy_record):.4f} (+/- {np.std(test_accuracy_record):.4f})')\n",
    "print(f'Average test loss per example and class over {k} folds: {total_test_loss / k:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_out_data_k(data, k=10):\n",
    "    total_count = len(data)\n",
    "    count_per_fold = total_count // k # Assumes cleanly divisble number\n",
    "    new_data = [0.0 for i in range(count_per_fold)]\n",
    "    for i in range(k):\n",
    "        for j in range(count_per_fold):\n",
    "            new_data[j] += data[i * count_per_fold + j]\n",
    "    new_data = [d / k for d in new_data]\n",
    "    return new_data\n",
    "\n",
    "def get_data_at_epoch_fold(data, epoch, total_num_folds=10):\n",
    "    # Returns a new list of data points at a specified epoch from all folds\n",
    "    # data = [fold1....fold10]\n",
    "    # epoch is 0-indexed\n",
    "    epoch_per_fold = len(data) // total_num_folds\n",
    "    new_data = [data[f * (epoch_per_fold) + epoch] for f in range(total_num_folds)]\n",
    "    return new_data # [epoch n from fold1, epoch n from fold2, ..., epoch n from fold10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "epochs_error_bar = [50, 100, 150, 200, 250, 300] # 1-indexed\n",
    "epochs_error_bar_0 = [epoch - 1 for epoch in epochs_error_bar] # 0-indexed\n",
    "\n",
    "# Plot Loss\n",
    "average_train_loss = average_out_data_k(train_loss_record)\n",
    "train_loss_error_bar_y = [average_train_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "train_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "# For Q6b_ii comparison\n",
    "batch_average_train_loss = average_train_loss \n",
    "batch_average_train_loss_error_bar_y = train_loss_error_bar_y\n",
    "batch_average_train_loss_error_bar_yerr = train_loss_error_bar_yerr\n",
    "\n",
    "average_valid_loss = average_out_data_k(holdout_loss_record)\n",
    "valid_loss_error_bar_y = [average_valid_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[0].plot(average_train_loss, '-b', label='Training loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=train_loss_error_bar_y, yerr=train_loss_error_bar_yerr, label='Training loss error', fmt='-b')\n",
    "\n",
    "axs[0].plot(average_valid_loss, '--r', label='Validation loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=valid_loss_error_bar_y, yerr=valid_loss_error_bar_yerr, label='Validation loss error', fmt='--r')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Normalized Loss')\n",
    "axs[0].set_title('Loss normalized per example and class vs. Epochs')\n",
    "\n",
    "# Plot Accuracy\n",
    "average_train_acc = average_out_data_k(train_accuracy_record)\n",
    "train_acc_error_bar_y = [average_train_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "train_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "average_valid_acc = average_out_data_k(holdout_accuracy_record)\n",
    "valid_acc_error_bar_y = [average_valid_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[1].plot(average_train_acc, '-b', label='Training accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=train_acc_error_bar_y, yerr=train_acc_error_bar_yerr, label='Training accuracy error', fmt='-b')\n",
    "\n",
    "axs[1].plot(average_valid_acc, '--r', label='Validation accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=valid_acc_error_bar_y, yerr=valid_acc_error_bar_yerr, label='Validation accuracy error', fmt='--r')\n",
    "\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_title('Accuracy vs. Epochs')\n",
    "\n",
    "plt.savefig(\"plots/Q6a_i.png\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (ii) Without PCA on aligned and with PCA on unaligned\n",
    "## 1. Without PCA on aligned\n",
    "\n",
    "print(\"(ii)\")\n",
    "print(\"Without PCA on aligned\")\n",
    "\n",
    "# Weights for Q6c (Weight Visualization on non PCA weights)\n",
    "weight_visualization_weights = None\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "# Softmax Regression Parameters\n",
    "lr = 0.01\n",
    "num_features = X.shape[1]\n",
    "num_classes = y.max() + 1\n",
    "\n",
    "train_loss_record = []\n",
    "\n",
    "train_accuracy_record = []\n",
    "\n",
    "holdout_loss_record = []\n",
    "\n",
    "holdout_accuracy_record = []\n",
    "\n",
    "test_accuracy_record = []\n",
    "\n",
    "num_epochs = 300\n",
    "epochs_print = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "k = 10\n",
    "\n",
    "total_test_accuracy = 0.0\n",
    "total_test_loss = 0.0\n",
    "\n",
    "cur_fold = 0\n",
    "for train, valid, test in generate_k_fold_set((X, y), k):\n",
    "    print(f\"Current Fold: {cur_fold}\")\n",
    "    train_data, train_label = train\n",
    "    valid_data, valid_label = valid\n",
    "    test_data, test_label = test\n",
    "    \n",
    "    train_d = append_bias(train_data)     \n",
    "    valid_d = append_bias(valid_data)\n",
    "    test_d = append_bias(test_data)\n",
    "\n",
    "    softmax_model = SoftmaxRegression(lr, num_features, num_classes)\n",
    "    best_w = softmax_model.W\n",
    "\n",
    "    # Onehot encode labels\n",
    "    y_true = onehot_encode(train_label)\n",
    "    valid_label_onehot = onehot_encode(valid_label)\n",
    "    test_label_onehot = onehot_encode(test_label)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_hat = softmax_model.model(train_d)\n",
    "        \n",
    "        raw_train_loss = softmax_model.cross_entropy(y_true, y_hat)\n",
    "\n",
    "        train_loss_norm = raw_train_loss / len(train_d) / num_classes # train loss normalized by # examples and classes\n",
    "        \n",
    "        train_loss_record.append(train_loss_norm)\n",
    "        \n",
    "        train_accuracy = softmax_model.accuracy(y_true, y_hat)\n",
    "        train_accuracy_record.append(train_accuracy)\n",
    "        \n",
    "        holdout_y = softmax_model.model(valid_d)\n",
    "\n",
    "        holdout_loss = softmax_model.cross_entropy(holdout_y, valid_label_onehot)\n",
    "        holdout_loss_norm = holdout_loss / len(valid_d) / num_classes # holdout loss normalized by # examples and classes\n",
    "        holdout_loss_record.append(holdout_loss_norm)\n",
    "\n",
    "        holdout_accuracy = softmax_model.accuracy(holdout_y, valid_label_onehot)\n",
    "        holdout_accuracy_record.append(holdout_accuracy)\n",
    "\n",
    "        # if holdout_accuracy >= max(holdout_accuracy_record):\n",
    "        if holdout_accuracy >= max(holdout_accuracy_record[cur_fold * num_epochs:]):\n",
    "            best_w = softmax_model.W\n",
    "            weight_visualization_weights = best_w[:-1]\n",
    "\n",
    "        # Update Weights\n",
    "        softmax_model.update_weights(train_d, y_true, y_hat)\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f' epoch: {epoch + 1}, train accuracy: {train_accuracy:.4f}, train_loss_norm:{train_loss_norm:.4f}, '\\\n",
    "                f'valid_acc: {holdout_accuracy:.4f}, valid_loss_norm: {holdout_loss_norm:.4f}')\n",
    "        else:\n",
    "            if (epoch + 1) in epochs_print:\n",
    "                print(f' epoch: {epoch + 1}, train accuracy: {train_accuracy:.4f}, train_loss_norm:{train_loss_norm:.4f}, '\\\n",
    "                    f'valid_acc: {holdout_accuracy:.4f}, valid_loss_norm: {holdout_loss_norm:.4f}')\n",
    "\n",
    "    # Run on Test Dataset\n",
    "    test_y = softmax_model.model_w(test_d, best_w)\n",
    "\n",
    "    test_accuracy = softmax_model.accuracy(test_y, test_label_onehot)\n",
    "\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    test_accuracy_record.append(test_accuracy)\n",
    "\n",
    "    raw_test_loss = softmax_model.cross_entropy(test_y, test_label_onehot)\n",
    "    test_loss_norm = raw_test_loss / len(test_d) / num_classes\n",
    "    total_test_loss += test_loss_norm\n",
    "    print(f\"Test loss norm: {test_loss_norm:.4f}\")\n",
    "\n",
    "    cur_fold += 1\n",
    "\n",
    "print(f'Average test accuracy over {k} folds: {np.mean(test_accuracy_record):.4f} (+/- {np.std(test_accuracy_record):.4f})')\n",
    "print(f'Average test loss per example and class over {k} folds: {total_test_loss / k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "epochs_error_bar = [50, 100, 150, 200, 250, 300] # 1-indexed\n",
    "epochs_error_bar_0 = [epoch - 1 for epoch in epochs_error_bar] # 0-indexed\n",
    "\n",
    "# Plot Loss\n",
    "average_train_loss = average_out_data_k(train_loss_record)\n",
    "train_loss_error_bar_y = [average_train_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "train_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "average_valid_loss = average_out_data_k(holdout_loss_record)\n",
    "valid_loss_error_bar_y = [average_valid_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[0].plot(average_train_loss, '-b', label='Training loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=train_loss_error_bar_y, yerr=train_loss_error_bar_yerr, label='Training loss error', fmt='-b')\n",
    "\n",
    "axs[0].plot(average_valid_loss, '--r', label='Validation loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=valid_loss_error_bar_y, yerr=valid_loss_error_bar_yerr, label='Validation loss error', fmt='--r')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Normalized Loss')\n",
    "axs[0].set_title('Loss normalized per example and class vs. Epochs')\n",
    "\n",
    "# Plot Accuracy\n",
    "average_train_acc = average_out_data_k(train_accuracy_record)\n",
    "train_acc_error_bar_y = [average_train_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "train_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "average_valid_acc = average_out_data_k(holdout_accuracy_record)\n",
    "valid_acc_error_bar_y = [average_valid_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[1].plot(average_train_acc, '-b', label='Training accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=train_acc_error_bar_y, yerr=train_acc_error_bar_yerr, label='Training accuracy error', fmt='-b')\n",
    "\n",
    "axs[1].plot(average_valid_acc, '--r', label='Validation accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=valid_acc_error_bar_y, yerr=valid_acc_error_bar_yerr, label='Validation accuracy error', fmt='--r')\n",
    "\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_title('Accuracy vs. Epochs')\n",
    "\n",
    "plt.savefig(\"plots/Q6a_ii_1.png\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. With PCA on unaligned\n",
    "\n",
    "print(\"With PCA on unaligned\")\n",
    "\n",
    "# Load unaligned data\n",
    "X, y = traffic_sign(False)\n",
    "X = X.astype(np.float32) # cast to float32 as float64 running out of memory\n",
    "X = z_score_normalize(X) \n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "# Softmax Regression Parameters\n",
    "lr = 0.5\n",
    "num_features = X.shape[1]\n",
    "num_classes = y.max() + 1\n",
    "\n",
    "train_loss_record = []\n",
    "\n",
    "train_accuracy_record = []\n",
    "\n",
    "holdout_loss_record = []\n",
    "\n",
    "holdout_accuracy_record = []\n",
    "\n",
    "test_accuracy_record = []\n",
    "\n",
    "# PCA number of principal components\n",
    "n_components = 300\n",
    "\n",
    "first_plot = True\n",
    "\n",
    "num_epochs = 300\n",
    "epochs_print = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "k = 10\n",
    "\n",
    "total_test_accuracy = 0.0\n",
    "total_test_loss = 0.0\n",
    "\n",
    "cur_fold = 0\n",
    "for train, valid, test in generate_k_fold_set((X, y), k):\n",
    "    print(f\"Current Fold: {cur_fold}\")\n",
    "    train_data, train_label = train\n",
    "    valid_data, valid_label = valid\n",
    "    test_data, test_label = test\n",
    "    \n",
    "    # Project data onto principal components\n",
    "    pca = PCA(n_components)\n",
    "    projected = pca.fit_transform(train_data) # len(train_data) x n_components\n",
    "    \n",
    "    # Plot principal components\n",
    "    if first_plot == True : \n",
    "        pca.plot_PC()\n",
    "        first_plot = False\n",
    "    train_d = append_bias(projected)     \n",
    "    valid_d = append_bias(pca.PCA_generate(valid_data))\n",
    "    test_d = append_bias(pca.PCA_generate(test_data))\n",
    "\n",
    "    softmax_model = SoftmaxRegression(lr, n_components, num_classes)\n",
    "    best_w = softmax_model.W\n",
    "\n",
    "    # Onehot encode labels\n",
    "    y_true = onehot_encode(train_label)\n",
    "    valid_label_onehot = onehot_encode(valid_label)\n",
    "    test_label_onehot = onehot_encode(test_label)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_hat = softmax_model.model(train_d)\n",
    "        \n",
    "        raw_train_loss = softmax_model.cross_entropy(y_true, y_hat)\n",
    "        train_loss_norm = raw_train_loss / len(train_d) / num_classes # train loss normalized by # examples and classes\n",
    "        \n",
    "        train_loss_record.append(train_loss_norm)\n",
    "        \n",
    "        train_accuracy = softmax_model.accuracy(y_true, y_hat)\n",
    "        train_accuracy_record.append(train_accuracy)\n",
    "        \n",
    "        holdout_y = softmax_model.model(valid_d)\n",
    "\n",
    "        holdout_loss = softmax_model.cross_entropy(holdout_y, valid_label_onehot)\n",
    "        holdout_loss_norm = holdout_loss / len(valid_d) / num_classes # holdout loss normalized by # examples and classes\n",
    "        holdout_loss_record.append(holdout_loss_norm)\n",
    "\n",
    "        holdout_accuracy = softmax_model.accuracy(holdout_y, valid_label_onehot)\n",
    "        holdout_accuracy_record.append(holdout_accuracy)\n",
    "\n",
    "        if holdout_accuracy >= max(holdout_accuracy_record[cur_fold * num_epochs:]):\n",
    "            best_w = softmax_model.W\n",
    "\n",
    "        # Update Weights\n",
    "        softmax_model.update_weights(train_d, y_true, y_hat)\n",
    "\n",
    "        if (epoch + 1) in epochs_print:\n",
    "            print(f' epoch: {epoch + 1}, train accuracy: {train_accuracy:.4f}, train_loss_norm:{train_loss_norm:.4f}, '\\\n",
    "                f'valid_acc: {holdout_accuracy:.4f}, valid_loss_norm: {holdout_loss_norm:.4f}')\n",
    "\n",
    "    # Run on Test Dataset\n",
    "    test_y = softmax_model.model_w(test_d, best_w)\n",
    "\n",
    "    test_accuracy = softmax_model.accuracy(test_y, test_label_onehot)\n",
    "\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    test_accuracy_record.append(test_accuracy)\n",
    "\n",
    "    raw_test_loss = softmax_model.cross_entropy(test_y, test_label_onehot)\n",
    "    test_loss_norm = raw_test_loss / len(test_d) / num_classes\n",
    "    total_test_loss += test_loss_norm\n",
    "    print(f\"Test loss norm: {test_loss_norm:.4f}\")\n",
    "\n",
    "    cur_fold += 1\n",
    "\n",
    "print(f'Average test accuracy over {k} folds: {np.mean(test_accuracy_record):.4f} (+/- {np.std(test_accuracy_record):.4f})')\n",
    "print(f'Average test loss per example and class over {k} folds: {total_test_loss / k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "epochs_error_bar = [50, 100, 150, 200, 250, 300] # 1-indexed\n",
    "epochs_error_bar_0 = [epoch - 1 for epoch in epochs_error_bar] # 0-indexed\n",
    "\n",
    "# Plot Loss\n",
    "average_train_loss = average_out_data_k(train_loss_record)\n",
    "train_loss_error_bar_y = [average_train_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "train_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "average_valid_loss = average_out_data_k(holdout_loss_record)\n",
    "valid_loss_error_bar_y = [average_valid_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[0].plot(average_train_loss, '-b', label='Training loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=train_loss_error_bar_y, yerr=train_loss_error_bar_yerr, label='Training loss error', fmt='-b')\n",
    "\n",
    "axs[0].plot(average_valid_loss, '--r', label='Validation loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=valid_loss_error_bar_y, yerr=valid_loss_error_bar_yerr, label='Validation loss error', fmt='--r')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Normalized Loss')\n",
    "axs[0].set_title('Loss normalized per example and class vs. Epochs')\n",
    "\n",
    "# Plot Accuracy\n",
    "average_train_acc = average_out_data_k(train_accuracy_record)\n",
    "train_acc_error_bar_y = [average_train_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "train_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "average_valid_acc = average_out_data_k(holdout_accuracy_record)\n",
    "valid_acc_error_bar_y = [average_valid_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[1].plot(average_train_acc, '-b', label='Training accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=train_acc_error_bar_y, yerr=train_acc_error_bar_yerr, label='Training accuracy error', fmt='-b')\n",
    "\n",
    "axs[1].plot(average_valid_acc, '--r', label='Validation accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=valid_acc_error_bar_y, yerr=valid_acc_error_bar_yerr, label='Validation accuracy error', fmt='--r')\n",
    "\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_title('Accuracy vs. Epochs')\n",
    "\n",
    "plt.savefig(\"plots/Q6a_ii_2.png\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (iii) Confusion Matrix on Test Set results\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6(b) - Stochastic Gradient Descent\n",
      "Cur fold: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27841, 300)\n",
      "(3479, 300)\n",
      "(3479, 300)\n",
      "0\n",
      " epoch: 1, train accuracy: 0.7941, train_loss_norm:0.0874, valid_acc: 0.7853, valid_loss_norm: 0.0575\n",
      "1\n",
      " epoch: 2, train accuracy: 0.7941, train_loss_norm:0.0873, valid_acc: 0.7853, valid_loss_norm: 0.0575\n",
      "2\n",
      " epoch: 3, train accuracy: 0.7941, train_loss_norm:0.0872, valid_acc: 0.7853, valid_loss_norm: 0.0575\n",
      "3\n",
      " epoch: 4, train accuracy: 0.7941, train_loss_norm:0.0872, valid_acc: 0.7853, valid_loss_norm: 0.0575\n",
      "4\n",
      " epoch: 5, train accuracy: 0.7941, train_loss_norm:0.0871, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "5\n",
      " epoch: 6, train accuracy: 0.7941, train_loss_norm:0.0870, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "6\n",
      " epoch: 7, train accuracy: 0.7941, train_loss_norm:0.0869, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "7\n",
      " epoch: 8, train accuracy: 0.7941, train_loss_norm:0.0868, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "8\n",
      " epoch: 9, train accuracy: 0.7941, train_loss_norm:0.0868, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "9\n",
      " epoch: 10, train accuracy: 0.7941, train_loss_norm:0.0867, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "10\n",
      " epoch: 11, train accuracy: 0.7941, train_loss_norm:0.0866, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "11\n",
      " epoch: 12, train accuracy: 0.7941, train_loss_norm:0.0865, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "12\n",
      " epoch: 13, train accuracy: 0.7941, train_loss_norm:0.0865, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "13\n",
      " epoch: 14, train accuracy: 0.7941, train_loss_norm:0.0864, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "14\n",
      " epoch: 15, train accuracy: 0.7941, train_loss_norm:0.0863, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "15\n",
      " epoch: 16, train accuracy: 0.7941, train_loss_norm:0.0862, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "16\n",
      " epoch: 17, train accuracy: 0.7941, train_loss_norm:0.0862, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "17\n",
      " epoch: 18, train accuracy: 0.7941, train_loss_norm:0.0861, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "18\n",
      " epoch: 19, train accuracy: 0.7941, train_loss_norm:0.0860, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "19\n",
      " epoch: 20, train accuracy: 0.7941, train_loss_norm:0.0859, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "20\n",
      " epoch: 21, train accuracy: 0.7941, train_loss_norm:0.0858, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "21\n",
      " epoch: 22, train accuracy: 0.7941, train_loss_norm:0.0858, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "22\n",
      " epoch: 23, train accuracy: 0.7941, train_loss_norm:0.0857, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "23\n",
      " epoch: 24, train accuracy: 0.7941, train_loss_norm:0.0856, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "24\n",
      " epoch: 25, train accuracy: 0.7941, train_loss_norm:0.0855, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "25\n",
      " epoch: 26, train accuracy: 0.7941, train_loss_norm:0.0855, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "26\n",
      " epoch: 27, train accuracy: 0.7942, train_loss_norm:0.0854, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "27\n",
      " epoch: 28, train accuracy: 0.7942, train_loss_norm:0.0853, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "28\n",
      " epoch: 29, train accuracy: 0.7942, train_loss_norm:0.0852, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "29\n",
      " epoch: 30, train accuracy: 0.7942, train_loss_norm:0.0852, valid_acc: 0.7856, valid_loss_norm: 0.0574\n",
      "30\n",
      " epoch: 31, train accuracy: 0.7942, train_loss_norm:0.0851, valid_acc: 0.7859, valid_loss_norm: 0.0573\n",
      "31\n",
      " epoch: 32, train accuracy: 0.7942, train_loss_norm:0.0850, valid_acc: 0.7859, valid_loss_norm: 0.0573\n",
      "32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10872/656036566.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mcur_ex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_d\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mcur_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_ex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m# Update Weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Victor\\OneDrive\\Documents\\UCSD\\0CSE251B\\hw1\\CSE251B\\cse251b_PA1_starter\\model\\softmax.py\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mModel\u001b[0m \u001b[0mNetwork\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mSoftmax\u001b[0m \u001b[0mRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         '''\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Q6(b) - Stochastic Gradient Descent\n",
    "\n",
    "print(\"Q6(b) - Stochastic Gradient Descent\")\n",
    "\n",
    "# (i)\n",
    "# Load aligned data\n",
    "X, y = traffic_sign(True)\n",
    "X = X.astype(np.float32) # cast to float32 as float64 running out of memory\n",
    "X = z_score_normalize(X) \n",
    "\n",
    "# Softmax Regression Parameters\n",
    "lr = 0.01\n",
    "num_features = X.shape[1]\n",
    "num_classes = y.max() + 1\n",
    "\n",
    "train_loss_record = []\n",
    "train_accuracy_record = []\n",
    "holdout_loss_record = []\n",
    "holdout_accuracy_record = []\n",
    "test_accuracy_record = []\n",
    "\n",
    "\n",
    "# PCA number of principal components\n",
    "n_components = 300\n",
    "\n",
    "first_plot = True\n",
    "\n",
    "num_epochs = 300\n",
    "epochs_print = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "cur_fold = 0\n",
    "\n",
    "for train, valid, test in generate_k_fold_set((X, y), k = 10):\n",
    "    print(\"Cur fold:\", cur_fold)\n",
    "    train_data, train_label = train\n",
    "    valid_data, valid_label = valid\n",
    "    test_data, test_label = test\n",
    "    \n",
    "    # Project data onto principal components\n",
    "    pca = PCA(n_components)\n",
    "    projected = pca.fit_transform(train_data)\n",
    "    \n",
    "    # Plot principal components\n",
    "    if first_plot == True : \n",
    "        pca.plot_PC()\n",
    "        first_plot = False\n",
    "    train_d = append_bias(projected)     \n",
    "    valid_d = append_bias(pca.PCA_generate(valid_data))\n",
    "    test_d = append_bias(pca.PCA_generate(test_data))\n",
    "\n",
    "    softmax_model = SoftmaxRegression(lr, n_components, num_classes)\n",
    "\n",
    "    valid_label_onehot = onehot_encode(valid_label)\n",
    "\n",
    "    # SGD\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle indices\n",
    "        indices = np.arange(len(train_d))\n",
    "        indices = np.random.shuffle(indices)\n",
    "\n",
    "        train_d = train_d[indices].squeeze(0)\n",
    "        train_label = train_label[indices].squeeze(0)\n",
    "        \n",
    "        # Onehot encode labels\n",
    "        y_true = onehot_encode(train_label)\n",
    "            \n",
    "        # Iterate over each example\n",
    "        for i in range(len(train_d)):\n",
    "            cur_ex = train_d[i][np.newaxis, :]\n",
    "            cur_label = y_true[i][np.newaxis, :]\n",
    "            y_hat = softmax_model.model(cur_ex)\n",
    "\n",
    "            # Update Weights\n",
    "            softmax_model.update_weights(cur_ex, cur_label, y_hat)\n",
    "\n",
    "        # Training Loss\n",
    "        y_hat = softmax_model.model(train_d)\n",
    "\n",
    "        raw_train_loss = softmax_model.cross_entropy(y_true, y_hat)\n",
    "        train_loss_norm = raw_train_loss / len(train_d) / num_classes\n",
    "\n",
    "        train_loss_record.append(raw_train_loss)\n",
    "\n",
    "        train_accuracy = softmax_model.accuracy(y_true, y_hat)\n",
    "        train_accuracy_record.append(train_accuracy)\n",
    "\n",
    "        # Validation Loss\n",
    "        holdout_y = softmax_model.model(valid_d)\n",
    "\n",
    "        holdout_loss = softmax_model.cross_entropy(holdout_y, valid_label_onehot)\n",
    "        holdout_loss_norm = holdout_loss / len(valid_d) / num_classes # holdout loss normalized by # examples and classes\n",
    "        holdout_loss_record.append(holdout_loss_norm)\n",
    "\n",
    "        holdout_accuracy = softmax_model.accuracy(holdout_y, valid_label_onehot)\n",
    "        holdout_accuracy_record.append(holdout_accuracy)\n",
    "\n",
    "        if holdout_accuracy >= max(holdout_accuracy_record[cur_fold * num_epochs:]):\n",
    "            best_w = softmax_model.W\n",
    "\n",
    "        \n",
    "        # if (epoch + 1) in epochs_print:\n",
    "        print(f' epoch: {epoch + 1}, train accuracy: {train_accuracy:.4f}, train_loss_norm:{train_loss_norm:.4f}, '\\\n",
    "            f'valid_acc: {holdout_accuracy:.4f}, valid_loss_norm: {holdout_loss_norm:.4f}')\n",
    "\n",
    "    # Run on Test Dataset\n",
    "    test_y = softmax_model.model_w(test_d, best_w)\n",
    "\n",
    "    test_label_onehot = onehot_encode(test_label)\n",
    "    test_accuracy = softmax_model.accuracy(test_y, test_label_onehot)\n",
    "\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    test_accuracy_record.append(test_accuracy)\n",
    "\n",
    "    raw_test_loss = softmax_model.cross_entropy(test_y, test_label_onehot)\n",
    "    test_loss_norm = raw_test_loss / len(test_d) / num_classes\n",
    "    total_test_loss += test_loss_norm\n",
    "    print(f\"Test loss norm: {test_loss_norm:.4f}\")\n",
    "\n",
    "    cur_fold += 1\n",
    "\n",
    "print(f'Average test accuracy over {k} folds: {np.mean(test_accuracy_record):.4f} (+/- {np.std(test_accuracy_record):.4f})')\n",
    "print(f'Average test loss per example and class over {k} folds: {total_test_loss / k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "epochs_error_bar = [50, 100, 150, 200, 250, 300] # 1-indexed\n",
    "epochs_error_bar_0 = [epoch - 1 for epoch in epochs_error_bar] # 0-indexed\n",
    "\n",
    "# Plot Loss\n",
    "average_train_loss = average_out_data_k(train_loss_record)\n",
    "train_loss_error_bar_y = [average_train_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "train_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "average_valid_loss = average_out_data_k(holdout_loss_record)\n",
    "valid_loss_error_bar_y = [average_valid_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[0].plot(average_train_loss, '-b', label='Training loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=train_loss_error_bar_y, yerr=train_loss_error_bar_yerr, label='Training loss error', fmt='-b')\n",
    "\n",
    "axs[0].plot(average_valid_loss, '--r', label='Validation loss')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=valid_loss_error_bar_y, yerr=valid_loss_error_bar_yerr, label='Validation loss error', fmt='--r')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Normalized Loss')\n",
    "axs[0].set_title('Loss normalized per example and class vs. Epochs')\n",
    "\n",
    "# Plot Accuracy\n",
    "average_train_acc = average_out_data_k(train_accuracy_record)\n",
    "train_acc_error_bar_y = [average_train_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "train_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "average_valid_acc = average_out_data_k(holdout_accuracy_record)\n",
    "valid_acc_error_bar_y = [average_valid_acc[epoch] for epoch in epochs_error_bar_0]\n",
    "valid_acc_error_bar_yerr = [np.std(get_data_at_epoch_fold(holdout_accuracy_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[1].plot(average_train_acc, '-b', label='Training accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=train_acc_error_bar_y, yerr=train_acc_error_bar_yerr, label='Training accuracy error', fmt='-b')\n",
    "\n",
    "axs[1].plot(average_valid_acc, '--r', label='Validation accuracy')\n",
    "axs[1].errorbar(x=epochs_error_bar_0, y=valid_acc_error_bar_y, yerr=valid_acc_error_bar_yerr, label='Validation accuracy error', fmt='--r')\n",
    "\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].set_title('Accuracy vs. Epochs')\n",
    "\n",
    "plt.savefig(\"plots/Q6b_i.png\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii) Plot Batch vs. Stochastic Gradient Descent\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "epochs_error_bar = [50, 100, 150, 200, 250, 300] # 1-indexed\n",
    "epochs_error_bar_0 = [epoch - 1 for epoch in epochs_error_bar] # 0-indexed\n",
    "\n",
    "# Plot Loss\n",
    "average_train_loss = average_out_data_k(train_loss_record)\n",
    "train_loss_error_bar_y = [average_train_loss[epoch] for epoch in epochs_error_bar_0]\n",
    "train_loss_error_bar_yerr = [np.std(get_data_at_epoch_fold(train_loss_record, epoch)) for epoch in epochs_error_bar_0]\n",
    "\n",
    "axs[0].plot(average_train_loss, '-b', label='Stochastic')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=train_loss_error_bar_y, yerr=train_loss_error_bar_yerr, label='Stochastic error', fmt='-b')\n",
    "\n",
    "axs[0].plot(batch_average_train_loss, '--r', label='Batch')\n",
    "axs[0].errorbar(x=epochs_error_bar_0, y=batch_average_train_loss_error_bar_y, yerr=batch_average_train_loss_error_bar_yerr, label='Batch error', fmt='--r')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Normalized Loss')\n",
    "axs[0].set_title('Batch vs. Stochastic Normalized Training Loss')\n",
    "\n",
    "plt.savefig(\"plots/Q6b_ii.png\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (c) Visualize the weights\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "def weights2range(w, min=0, max=256):\n",
    "    # Scales weights to be between min and max\n",
    "    w /= np.max(w)\n",
    "    w *= (max)\n",
    "    w += min\n",
    "    return w\n",
    "\n",
    "# Plot image\n",
    "\n",
    "weight_visualization_weights = best_w[:-1]\n",
    "\n",
    "# Traffic sign classes to be plotted\n",
    "ts_classes = [7, 11, 21, 25]\n",
    "\n",
    "fig, axs = plt.subplots(4,1) \n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "for i, ts_class in enumerate(ts_classes):\n",
    "    # Get weights for each class\n",
    "    class_weights = weight_visualization_weights[:,ts_class].reshape((32, 32))\n",
    "    # Scale to [0, 256]\n",
    "    img_weights = weights2range(class_weights)\n",
    "    # Plot image\n",
    "    axs[i].set_title(f'Class {ts_class}')\n",
    "    axs[i].imshow(img_weights)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/Q6c_weights.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "005f4ed24810181fa8b2266ec303702d6575bedfaa41c7848d327e7f780b3129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
